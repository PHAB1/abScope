import pandas as pd
import numpy as np
import uuid
import os
import shutil
import glob
import subprocess
import time
from Bio import SeqIO
import argparse
import concurrent.futures

parser = argparse.ArgumentParser(description="Process a file based in the barcode string.")
parser.add_argument('barcode', type=str, help="The barcode to be processed")
parser.add_argument('chain_type', type=str, help="The chain to be processed")
args = parser.parse_args()

min_seqs_cluster = 6
cdr3_ident = 0.85
output_cdr3_clusters_dir = "cdr3_clusters"
permissions = 0o555  # Read and execute for owner, group, and others

try:
    os.makedirs(output_cdr3_clusters_dir)
except FileExistsError:
    shutil.rmtree(output_cdr3_clusters_dir)
    os.makedirs(output_cdr3_clusters_dir)

news_clusters_dir = "new_clusters"
news_clusters_cdr3_dir = "%s/new_cdr3_clusters"%news_clusters_dir
news_final_clusters_dir = "%s/new_final_clusters"%news_clusters_dir

## Input Files ##
chain_type = args.chain_type # VH or VL

# Consensus file
original_consensus_file = "%s_vsearch_consensus.fasta"%args.barcode

## Output File ##
final_consensus = "%s_processed.fasta"%args.barcode

# read igblast alignments file
igblast1 = pd.read_csv("igCicle/1_igCicle.tsv",sep="\t", low_memory=False)
igblast2 = pd.read_csv("igCicle/2_igCicle.tsv",sep="\t", low_memory=False)

igblast = pd.concat([igblast1, igblast2], axis=0)
igblast = igblast.drop_duplicates(subset=['sequence_id', 'locus'], keep='first') # remove duplicates with same region VH or VL

if chain_type == "VH":
    igblast = igblast[igblast['locus']=='IGH']
else:
    igblast = igblast[igblast['locus']!='IGH']

# Clusters File
clusters = pd.read_csv("clusters.uc",sep="\t",header=None)

try: 
    os.makedirs("clusters_consensus")
except FileExistsError:
    shutil.rmtree("clusters_consensus")
    os.makedirs("clusters_consensus")

# For each consensus generated by vsearch put consensus in a separated directory to be used as medaka draft
for rec in SeqIO.parse(original_consensus_file, "fasta"):
    SeqIO.write(rec, "clusters_consensus/%s.fasta"%rec.id.split(";")[0].split("=")[-1], "fasta")

# Create clusters directories 
try:
    os.makedirs(news_clusters_dir)
    os.makedirs(news_clusters_cdr3_dir)
    os.makedirs(news_final_clusters_dir)
except FileExistsError:
    shutil.rmtree(news_clusters_dir)
    os.makedirs(news_clusters_dir)
    os.makedirs(news_clusters_cdr3_dir)
    os.makedirs(news_final_clusters_dir)

# Get clusters mapping using C for get each cluster respresentant sequence ID and H for the sequences in each cluster
match_id_clusters = clusters[clusters.iloc[:, 0] == "H"].iloc[:,8:]
filtered_clusters = clusters[clusters.iloc[:, 0] == "C"]
filtered_clusters = filtered_clusters[filtered_clusters[2] > min_seqs_cluster]
filtered_clusters = filtered_clusters.sort_values(by=2, ascending=False)
print("First file Clusterized Ordened By clusters lenght: \n%s"%filtered_clusters)


## cluster seq to original id ##
cluster_seq_id = {
    "cluster": [],
    "seq":[]
}

for clus_id in filtered_clusters[8]:
    seqs_clus = match_id_clusters[match_id_clusters[9] == clus_id][8]
    merge_clus_ig = pd.merge(seqs_clus, igblast, left_on=8, right_on='sequence_id', how='inner')
    output_cdrs_file = open("%s/%s.fasta"%(output_cdr3_clusters_dir,clus_id),"w")
    for rec in merge_clus_ig.iloc:
        output_cdrs_file.write(">%s\n%s\n"%(rec["sequence_id"], rec["cdr3"]))
        cluster_seq_id["cluster"].append(clus_id)
        cluster_seq_id["seq"].append(rec["sequence_id"])

    output_cdrs_file.close()

cluster_seq_id = pd.DataFrame(cluster_seq_id)

# vsearch CDR3 clusterization
for file in glob.glob("%s/*.fasta"%output_cdr3_clusters_dir):
    output_file_name = file.split("/")[-1].split(".")[0]
    subprocess.run(["vsearch", "--cluster_fast", "../../%s"%file, "--id", str(cdr3_ident), "--target_cov", "0.9","--minseqlength", "6", "--uc", "%s.uc"%output_file_name, "--consout", "%s.fasta"%output_file_name ],cwd=news_clusters_cdr3_dir)

time.sleep(1)
dict_new_cluster_id = {
    'seq': [],
    'cdr_cluster_id': []
}

# Dictionary to get original draft for medaka consensus
for file in glob.glob("%s/*.uc"%news_clusters_cdr3_dir):
    try:
        cluster = pd.read_csv(file,sep="\t",header=None)
    except:
        continue
    if cluster.empty:
        continue
    match_id_cluster = cluster[cluster.iloc[:, 0] == "H"].iloc[:,8:]
    filtered_cluster = cluster[cluster.iloc[:, 0] == "C"]
    filtered_cluster = filtered_cluster[filtered_cluster[2] > min_seqs_cluster]
    filtered_cluster = filtered_cluster.sort_values(by=2, ascending=False)
    print("Clusters Ordened By clusters lenght: \n%s"%filtered_cluster)
    for clus_id in filtered_cluster[8]:
        seqs_clus = match_id_cluster[match_id_cluster[9] == clus_id][8]
        dict_new_cluster_id["seq"].extend(seqs_clus)
        dict_new_cluster_id["cdr_cluster_id"].extend([clus_id]*len(seqs_clus))
        
        merge_clus_ig = pd.merge(seqs_clus, igblast, left_on=8, right_on='sequence_id', how='inner')
        output_cluster_file = open("%s/%s.fasta"%(news_final_clusters_dir,clus_id.split(";")[0]),"w")

        rec_id_dic = {}
        for rec in merge_clus_ig.iloc:

            if rec["sequence_id"] in rec_id_dic.keys():
                rec.id = "%s_%s"%(rec["sequence_id"], rec_id_dic[rec["sequence_id"]])
            else:
                rec.id = rec["sequence_id"]
                
            try:
                rec_id_dic[rec["sequence_id"]] += 1
            except:
                rec_id_dic[rec["sequence_id"]] = 1

            output_cluster_file.write(">%s\n%s\n"%(rec.id, rec["sequence"]))
        
        output_cluster_file.close()

df_new_cluster_id = pd.DataFrame(dict_new_cluster_id)
df_new_cluster_id = pd.merge(cluster_seq_id,df_new_cluster_id, left_on='seq', right_on='seq', how='left')
df_new_cluster_id.to_csv("meta_%s.csv"%args.chain_type, index=False)
#cluster_seq_id cluster, seq

# Medaka consensus
medaka_output = "medaka_consensus"
try:
    os.makedirs(medaka_output)
except FileExistsError:
    shutil.rmtree(medaka_output)
    os.makedirs(medaka_output)

final_consensus = open(final_consensus, "w")
medaka_output = "medaka_consensus"

# Medaka paralelization test
def process_file(file):
    seq_id = file.split(".")[0].split("/")[-1].split("_")[0]
    cluster_id = cluster_seq_id[cluster_seq_id["seq"] == seq_id].iloc[0]["cluster"]
    cluster_id = cluster_id.split("_")[0]
    
    subprocess.run(["medaka_consensus", "-g", "-d", f"clusters_consensus/{cluster_id}.fasta", "-i", file, "-t", "6", "-o", f"{medaka_output}/{seq_id}"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    try:
        dup_count = len(SeqIO.to_dict(SeqIO.parse(file, "fasta")))
    except ValueError as e:
        print(e)
        return None

    subprocess.run(["medaka_haploid_variant", "-i", f"clusters_consensus/{cluster_id}.fasta", "-r", f"{medaka_output}/{seq_id}/consensus.fasta", "-o", f"{medaka_output}/{seq_id}/variant_call/"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    #medaka_cons_seq = (SeqIO.read(f"{medaka_output}/{seq_id}/consensus.fasta", "fasta")).seq
    records = list(SeqIO.parse(f"{medaka_output}/{seq_id}/consensus.fasta", "fasta"))
    medaka_cons_seq = records[0].seq
    return f">{seq_id}|DUPCOUNT={dup_count}\n{medaka_cons_seq}\n"

with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:
    results = executor.map(process_file, glob.glob(f"{news_final_clusters_dir}/*.fasta"))

for result in results:
    if result:
        final_consensus.write(result)

final_consensus.close()

